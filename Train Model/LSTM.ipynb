{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import html\n",
    "\n",
    "import my_utils\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Prepare Data\n",
    "\n",
    "The data format expected for a LSTM network is not the same as what we previously used for NB model. LSTM is expecting sequence input, so we have to make our data into the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_data = pd.read_csv('sampled_data.csv')\n",
    "kindle_data_sample = kindle_data.sample(frac=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even if you are using the whole dataset, please make sure you do the shuffle. Otherwise the mini-batch traning will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2537"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kindle_data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = sw.words('english')\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "\n",
    "def preprocessing(line):\n",
    "    line = html.unescape(str(line))\n",
    "    line = str(line).translate(translation)\n",
    "    line = word_tokenize(line.lower())\n",
    "    \n",
    "    line = [lemmatizer.lemmatize(t) for t in line if t not in stop]\n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "n = len(kindle_data_sample)\n",
    "for i in range(n):\n",
    "    data.append(preprocessing(kindle_data['reviewText'][i]))\n",
    "    my_utils.print_progress(i + 1, n, decimals=0, bar_length=50)\n",
    "\n",
    "labels = [1 if x == 'pos' else 0 for x in kindle_data_sample['overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vocabulary\n",
    "\n",
    "Since we're going to encoding text into word index sequence, we need to build a map between word and index in vocabulary (*usually from the most frequent one to the least frequent one*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [w for line in data for w in line.split()]\n",
    "freqdist = FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdist_10 = {k:v for k, v in freqdist.items() if v >= 50}\n",
    "len(freqdist_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 3189),\n",
       " ('read', 1599),\n",
       " ('story', 1572),\n",
       " ('one', 939),\n",
       " ('love', 834),\n",
       " ('character', 783),\n",
       " ('great', 740),\n",
       " ('good', 708),\n",
       " ('like', 657),\n",
       " ('well', 585)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCA_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = {word[0]: idx + 1\n",
    "              for idx, word in enumerate(list(freqdist.most_common(VOCA_SIZE)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding(text, vocab, max_len=100):\n",
    "    res = []\n",
    "    \n",
    "    # Text is preprocessed and splitable by white space\n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "            res.append(vocab[word])\n",
    "    # Fix lenghth\n",
    "    if len(res) > max_len:\n",
    "        res = res[:max_len]\n",
    "    else:\n",
    "        res = [0] * (max_len - len(res)) + res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 41.5534095388\n",
      "Max length: 663\n",
      "Min length: 2\n",
      "StdDev: 53.061342941\n"
     ]
    }
   ],
   "source": [
    "# Statistics on sequence length\n",
    "seq_len = np.asarray([len(line.split()) for line in data])\n",
    "print('Average length:', np.average(seq_len))\n",
    "print('Max length:', np.max(seq_len))\n",
    "print('Min length:', np.min(seq_len))\n",
    "print('StdDev:', np.std(seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding data: |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(data)):\n",
    "    X.append(encoding(data[i], vocabulary, max_len=SEQ_MAX_LEN))\n",
    "    my_utils.print_progress(i + 1, len(data), decimals=0, bar_length=50, prefix='Encoding data:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"amazon_sample_data.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"X\", data=X)\n",
    "    f.create_dataset(\"Y\", data=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = h5py.File('amazon_sample_data.h5', \"r\")\n",
    "    X = np.array(dataset[\"X\"][:])\n",
    "    Y = np.array(dataset[\"Y\"][:])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (2537, 50)\n",
      "Y (2537,)\n"
     ]
    }
   ],
   "source": [
    "# Print shape\n",
    "print('X', X.shape)\n",
    "print('Y', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Parameters\n",
    "LSTM_UNITS = 100\n",
    "DROP_OUT = 0.3\n",
    "RNN_DROP_OUT = 0.2\n",
    "WORD_VEC_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Word Embedding Layer\n",
    "model.add(Embedding(\n",
    "    input_dim=VOCA_SIZE + 1, \n",
    "    output_dim=WORD_VEC_LEN))\n",
    "\n",
    "# LSTM Layer\n",
    "model.add(LSTM(\n",
    "    LSTM_UNITS, \n",
    "    dropout=DROP_OUT,\n",
    "    recurrent_dropout=RNN_DROP_OUT))\n",
    "\n",
    "# FC Layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          19264     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 85,365\n",
      "Trainable params: 85,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=losses.binary_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "N_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, Y, \n",
    "    epochs=N_EPOCH,\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(\n",
    "    input_dim= VOCA_SIZE + 1,\n",
    "    output_dim=WORD_VEC_LEN))\n",
    "\n",
    "model2.add(Conv1D(\n",
    "    filters=32, \n",
    "    kernel_size=3,\n",
    "    padding='same', \n",
    "    activation='relu'))\n",
    "\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model2.add(LSTM(\n",
    "    LSTM_UNITS, \n",
    "    dropout=0.2, \n",
    "    recurrent_dropout=0.2))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=[metrics.binary_accuracy])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    X, Y, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
